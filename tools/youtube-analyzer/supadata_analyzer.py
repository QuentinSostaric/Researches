#!/usr/bin/env python3
"""
Script pour r√©cup√©rer les vid√©os et transcriptions d'une cha√Æne YouTube
via l'API Supadata.ai
"""

import json
import sys
import time
import urllib.request
import urllib.error
import ssl

# Configuration
API_KEY = "sd_c6d07c43a85f83c938383a2ff08a41cb"
BASE_URL = "https://api.supadata.ai/v1"

# D√©sactiver la v√©rification SSL (pour contourner le proxy)
ssl._create_default_https_context = ssl._create_unverified_context

def api_request(endpoint, params=None):
    """Effectue une requ√™te √† l'API Supadata"""
    url = f"{BASE_URL}{endpoint}"

    if params:
        query_string = "&".join([f"{k}={urllib.parse.quote(str(v))}" for k, v in params.items()])
        url = f"{url}?{query_string}"

    headers = {
        "x-api-key": API_KEY,
        "Accept": "application/json",
        "User-Agent": "Mozilla/5.0"
    }

    req = urllib.request.Request(url, headers=headers)

    try:
        with urllib.request.urlopen(req, timeout=60) as response:
            return json.loads(response.read().decode('utf-8'))
    except urllib.error.HTTPError as e:
        print(f"Erreur HTTP {e.code}: {e.reason}")
        try:
            error_body = e.read().decode('utf-8')
            print(f"D√©tails: {error_body}")
        except:
            pass
        return None
    except Exception as e:
        print(f"Erreur: {e}")
        return None

def get_channel_videos(channel_url, video_type="all", limit=100):
    """R√©cup√®re la liste des vid√©os d'une cha√Æne"""
    print(f"\nüì∫ R√©cup√©ration des vid√©os de: {channel_url}")

    result = api_request("/youtube/channel/videos", {
        "id": channel_url,
        "type": video_type,
        "limit": limit
    })

    if result:
        video_ids = result.get("videoIds", [])
        short_ids = result.get("shortIds", [])
        live_ids = result.get("liveIds", [])

        print(f"‚úÖ Vid√©os trouv√©es: {len(video_ids)}")
        print(f"‚úÖ Shorts trouv√©s: {len(short_ids)}")
        print(f"‚úÖ Lives trouv√©s: {len(live_ids)}")

        return {
            "videos": video_ids,
            "shorts": short_ids,
            "lives": live_ids
        }

    return None

def get_video_transcript(video_id, lang="en"):
    """R√©cup√®re la transcription d'une vid√©o"""
    video_url = f"https://www.youtube.com/watch?v={video_id}"

    result = api_request("/youtube/transcript", {
        "url": video_url,
        "lang": lang
    })

    if result:
        return result

    # Essayer sans sp√©cifier la langue
    result = api_request("/youtube/transcript", {
        "url": video_url
    })

    return result

def get_video_metadata(video_id):
    """R√©cup√®re les m√©tadonn√©es d'une vid√©o"""
    video_url = f"https://www.youtube.com/watch?v={video_id}"

    result = api_request("/youtube/video", {
        "url": video_url
    })

    return result

def format_timecode(ms):
    """Convertit des millisecondes en timecode MM:SS"""
    seconds = ms / 1000
    minutes = int(seconds // 60)
    secs = int(seconds % 60)
    return f"{minutes:02d}:{secs:02d}"

def analyze_channel(channel_url, output_dir="."):
    """Analyse compl√®te d'une cha√Æne YouTube"""
    print("=" * 80)
    print(f"üîç Analyse de la cha√Æne: {channel_url}")
    print("=" * 80)

    # √âtape 1: R√©cup√©rer la liste des vid√©os
    videos_data = get_channel_videos(channel_url, limit=200)

    if not videos_data:
        print("‚ùå Impossible de r√©cup√©rer les vid√©os")
        return None

    all_video_ids = videos_data["videos"] + videos_data["shorts"]

    # Sauvegarder la liste des vid√©os
    videos_file = f"{output_dir}/video-list.json"
    with open(videos_file, 'w', encoding='utf-8') as f:
        json.dump(videos_data, f, ensure_ascii=False, indent=2)
    print(f"\nüíæ Liste des vid√©os sauvegard√©e: {videos_file}")

    # √âtape 2: R√©cup√©rer les m√©tadonn√©es et transcriptions
    results = {
        "channel_url": channel_url,
        "total_videos": len(all_video_ids),
        "videos": []
    }

    print(f"\nüìù R√©cup√©ration des transcriptions ({len(all_video_ids)} vid√©os)...")

    for i, video_id in enumerate(all_video_ids):
        print(f"\n[{i+1}/{len(all_video_ids)}] Vid√©o: {video_id}")

        video_data = {
            "id": video_id,
            "url": f"https://www.youtube.com/watch?v={video_id}",
            "title": None,
            "transcript": None,
            "full_text": None
        }

        # R√©cup√©rer les m√©tadonn√©es
        metadata = get_video_metadata(video_id)
        if metadata:
            video_data["title"] = metadata.get("title", "Sans titre")
            video_data["duration"] = metadata.get("duration")
            video_data["description"] = metadata.get("description")
            print(f"  üìå Titre: {video_data['title'][:60]}...")

        # R√©cup√©rer la transcription
        transcript = get_video_transcript(video_id)
        if transcript and "content" in transcript:
            video_data["transcript"] = transcript["content"]
            video_data["language"] = transcript.get("lang", "unknown")

            # Cr√©er le texte complet
            full_text = " ".join([item.get("text", "") for item in transcript["content"]])
            video_data["full_text"] = full_text

            print(f"  ‚úÖ Transcription r√©cup√©r√©e ({len(transcript['content'])} segments)")
        else:
            print(f"  ‚ö†Ô∏è Pas de transcription disponible")

        results["videos"].append(video_data)

        # Pause pour √©viter le rate limiting
        time.sleep(0.5)

    # Sauvegarder les r√©sultats
    transcripts_file = f"{output_dir}/transcripts.json"
    with open(transcripts_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\n{'=' * 80}")
    print(f"‚úÖ Analyse termin√©e!")
    print(f"üíæ Transcriptions sauvegard√©es: {transcripts_file}")

    # Statistiques
    with_transcript = sum(1 for v in results["videos"] if v["transcript"])
    print(f"\nüìä Statistiques:")
    print(f"   - Vid√©os analys√©es: {len(results['videos'])}")
    print(f"   - Avec transcription: {with_transcript}")
    print(f"   - Sans transcription: {len(results['videos']) - with_transcript}")

    return results

if __name__ == "__main__":
    import urllib.parse

    if len(sys.argv) < 2:
        print("Usage: python supadata_analyzer.py <channel_url> [output_dir]")
        print()
        print("Exemple:")
        print("  python supadata_analyzer.py https://www.youtube.com/@TheMasculineHome ./output")
        sys.exit(1)

    channel_url = sys.argv[1]
    output_dir = sys.argv[2] if len(sys.argv) > 2 else "."

    analyze_channel(channel_url, output_dir)
